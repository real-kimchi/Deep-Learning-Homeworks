{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099fe392",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Darren Liu dl499\n",
    "#SYSEN6888 Deeplearning Homework 2 Problem 1\n",
    "#October 13 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844e394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 23:40:04.043831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57612 images belonging to 131 classes.\n",
      "Found 10080 images belonging to 131 classes.\n",
      "Found 22688 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "#preprocessing fruitdata\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy\n",
    "\n",
    "# Create an ImageDataGenerator instance for data normalization and augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.15,  # 15% of the data will be used for validation\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "# Training and validation generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/kommanderkimchi/Desktop/Deep Learning/ConvNet HW 2/fruits-360_dataset/fruits-360/Training',\n",
    "    target_size=(75, 75),\n",
    "    batch_size=1000,\n",
    "    class_mode='categorical',\n",
    "    subset='training',  # Use the training data\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/kommanderkimchi/Desktop/Deep Learning/ConvNet HW 2/fruits-360_dataset/fruits-360/Training',\n",
    "    target_size=(75, 75),\n",
    "    batch_size=1000,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Use the validation data\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/Users/kommanderkimchi/Desktop/Deep Learning/ConvNet HW 2/fruits-360_dataset/fruits-360/Test',\n",
    "    target_size=(75, 75),\n",
    "    batch_size=1000,\n",
    "    class_mode='categorical',\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b92ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 73, 73, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 71, 71, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 71, 71, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 71, 71, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 35, 35, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 156800)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               40141056  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 131)               33667     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,250,883\n",
      "Trainable params: 40,250,627\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model architecture\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers as per the given architecture\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='valid', activation='relu', input_shape=(75, 75, 3), kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='valid', activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "model.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "model.add(Dropout(rate=0.3, seed=99))  # We'll experiment with this rate later\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "model.add(Dense(131, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "\n",
    "# Show the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8cb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 23:40:12.013139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/57 [=========================>....] - ETA: 1:11:49 - loss: 2.6630 - accuracy: 0.7029"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20, \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test steps\n",
    "test_steps = test_generator.samples // test_generator.batch_size\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db97456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4e961",
   "metadata": {},
   "source": [
    "Discussion and Analysis\n",
    "\n",
    "Model Accuracy: This will show how the accuracy changes over epochs for both the training and validation sets.\n",
    "Model Loss: This will show how the loss changes over epochs for both the training and validation sets.\n",
    "Now, regarding the analysis:\n",
    "\n",
    "Functions of Dropout Values: Dropout helps in reducing overfitting. The higher the dropout rate, the more regularization is added, but it may also slow down the learning process.\n",
    "\n",
    "Number of Convolutional Layers: Adding more convolutional layers allows the model to learn more complex features. However, it also increases the risk of overfitting, especially if the dataset is not large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700fd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "##skip connect model\n",
    "\n",
    "from tensorflow.keras.layers import Add, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(75, 75, 3))\n",
    "\n",
    "# First Convolutional Block\n",
    "x1 = Conv2D(64, (3, 3), strides=(1, 1), padding='valid', activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x1)\n",
    "\n",
    "# Second Convolutional Block\n",
    "x2 = Conv2D(128, (3, 3), strides=(1, 1), padding='valid', activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros')(x)\n",
    "x = BatchNormalization(momentum=0.99, epsilon=0.001)(x2)\n",
    "x = Dropout(rate=0.3)(x)  # We'll stick with a 0.3 dropout rate for this example\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x)\n",
    "\n",
    "# Adding Skip Connection (Add the output of the first Conv layer to the output of the second Conv layer)\n",
    "x = Add()([x, x1])\n",
    "\n",
    "# Following the rest of the architecture\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros')(x)\n",
    "output_layer = Dense(131, activation='softmax', kernel_initializer='glorot_uniform', bias_initializer='zeros')(x)\n",
    "\n",
    "# Create the model with skip connection\n",
    "model_with_skip = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Show the model summary\n",
    "model_with_skip.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667430dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with skip connection\n",
    "model_with_skip.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with skip connection\n",
    "history_with_skip = model_with_skip.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,  # You can change this to 20 if it's too time-consuming\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# Evaluate the model with skip connection on the test data\n",
    "test_loss_with_skip, test_accuracy_with_skip = model_with_skip.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "print(f\"Test Loss with Skip Connection: {test_loss_with_skip}\")\n",
    "print(f\"Test Accuracy with Skip Connection: {test_accuracy_with_skip * 100}%\")\n",
    "\n",
    "# Plotting for the model with skip connection\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_with_skip.history['accuracy'])\n",
    "plt.plot(history_with_skip.history['val_accuracy'])\n",
    "plt.title('Model with Skip Connection - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_with_skip.history['loss'])\n",
    "plt.plot(history_with_skip.history['val_loss'])\n",
    "plt.title('Model with Skip Connection - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
